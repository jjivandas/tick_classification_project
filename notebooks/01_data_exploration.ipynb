{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4a7cb37",
   "metadata": {},
   "source": [
    "## Setting up imports and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48acb592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "import unicodedata\n",
    "from typing import Optional\n",
    "import importlib.metadata as m, json\n",
    "ver = m.version(\"pybioclip\")\n",
    "print(\"pybioclip:\", ver)\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the paths to your data\n",
    "PROCESSED_DATA_DIR = Path('../data/processed/')\n",
    "METADATA_PATH = '../data/raw/AI Image Data Sept 3 2025.csv'\n",
    "IMAGE_DIR = Path(\"/Users/jayjivandas/Library/CloudStorage/OneDrive-TheOhioStateUniversity/Research-Onedrive/Imageomics/Bryant, Cece's files - Tick Images - Copy copy\")\n",
    "# Define JSON paths more explicitly\n",
    "FULL_DATA_JSON = os.path.join(PROCESSED_DATA_DIR, \"final_data.json\")\n",
    "FILTERED_DATA_JSON = os.path.join(PROCESSED_DATA_DIR, \"final_data_no_ixodes.json\")\n",
    "CLASS_NAMES_JSON = os.path.join(PROCESSED_DATA_DIR, \"class_names.json\")\n",
    "FILTERED_CLASS_NAMES_JSON = os.path.join(PROCESSED_DATA_DIR, \"class_names_no_ixodes.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebb4bd0",
   "metadata": {},
   "source": [
    "## Loading in the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "91637293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 665 records from the metadata file.\n",
      "Found 1334 image files to search through.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tc/csl5ykwn1v36vwqf20swsn880000gn/T/ipykernel_67706/4201957005.py:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the data from your CSV into a DataFrame.\n",
    "df = pd.read_csv(METADATA_PATH)\n",
    "df.fillna('', inplace=True)\n",
    "# removing duplicates based on 'Sample ID'\n",
    "df.drop_duplicates(subset=['Sample ID'], keep='first', inplace=True)\n",
    "\n",
    "# 2. Get All Available Image Filenames\n",
    "# Get a set of all available image filenames for fast lookups.\n",
    "# We convert all filenames to uppercase to handle potential inconsistencies (e.g., .jpg vs .JPG).\n",
    "all_image_files = {f.upper(): f for f in os.listdir(IMAGE_DIR)}\n",
    "\n",
    "print(f\"Loaded {len(df)} records from the metadata file.\")\n",
    "print(f\"Found {len(all_image_files)} image files to search through.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84f0dd7",
   "metadata": {},
   "source": [
    "## Determining the final list of valid ids\n",
    "### cross referencing the csv specimens to that of the images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34e357df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Determine the Final List of Valid IDs ---\n",
    "csv_ids = set(df['Sample ID'].astype(str)) # only unique IDs from the CSV\n",
    "image_base_ids = {f.upper().rsplit('-', 1)[0] for f in all_image_files}\n",
    "valid_ids = csv_ids.intersection(image_base_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eabe4010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated. Proceeding to create JSON for 664 specimens.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Filter DataFrame to Final Valid Set ---\n",
    "final_df = df[df['Sample ID'].isin(valid_ids)].copy()\n",
    "print(f\"Validated. Proceeding to create JSON for {len(final_df)} specimens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "67333bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata\n",
    "\n",
    "def normalize_spaces(s):\n",
    "    s = unicodedata.normalize(\"NFKC\", s or \"\")\n",
    "    s = \"\".join(\" \" if unicodedata.category(ch).startswith(\"Z\") else ch for ch in s)  # fixes \\u00A0\n",
    "    s = \"\".join(ch for ch in s if unicodedata.category(ch) not in {\"Cc\",\"Cf\"})        # drops zero-widths\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "DERM_VARIABLIS = re.compile(r\"^\\s*dermacentor\\s+variablis\\s*$\", re.I)\n",
    "\n",
    "def fix_label_minimal(raw):\n",
    "    s = normalize_spaces(raw)\n",
    "    if DERM_VARIABLIS.match(s):\n",
    "        return \"Dermacentor variabilis\"\n",
    "    parts = s.split()\n",
    "    if len(parts) >= 2:\n",
    "        return parts[0].capitalize() + \" \" + parts[1].lower()  # Genus species\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b52b2c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBSP left: 0\n",
      "'variablis' left: 0\n"
     ]
    }
   ],
   "source": [
    "final_df[\"true_label\"] = final_df[\"Species of Tick\"].astype(str).apply(fix_label_minimal)\n",
    "\n",
    "# quick checks\n",
    "print(\"NBSP left:\", final_df[\"true_label\"].str.contains(\"\\u00A0\", regex=False).sum())\n",
    "print(\"'variablis' left:\", (final_df[\"true_label\"].str.lower() == \"dermacentor variablis\").sum())\n",
    "\n",
    "# optional peek\n",
    "# print(sorted(final_df[\"true_label\"].unique()))\n",
    "# print(final_df[\"true_label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae9e55f",
   "metadata": {},
   "source": [
    "## Building the JSON structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "03b8b582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1324 total entries for the JSON file.\n",
      "Missing images for 2 specimens: ['246-01', '33-01']\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Build the JSON Structure ---\n",
    "final_data_list = []\n",
    "class_names_set = set()\n",
    "missing_ids = []\n",
    "\n",
    "for index, row in final_df.iterrows():\n",
    "    base_id = row['Sample ID'].upper()\n",
    "    dorsal_path, ventral_path = None, None\n",
    "\n",
    "    dorsal_patterns = [f\"{base_id}-01.JPG\", f\"{base_id}-1.JPG\"]\n",
    "    ventral_patterns = [f\"{base_id}-02.JPG\", f\"{base_id}-2.JPG\"]\n",
    "\n",
    "    for pattern in dorsal_patterns:\n",
    "        if pattern in all_image_files:\n",
    "            original_filename = all_image_files[pattern]\n",
    "            dorsal_path = os.path.join(IMAGE_DIR, original_filename)\n",
    "            break\n",
    "    for pattern in ventral_patterns:\n",
    "        if pattern in all_image_files:\n",
    "            original_filename = all_image_files[pattern]\n",
    "            ventral_path = os.path.join(IMAGE_DIR, original_filename)\n",
    "            break\n",
    "\n",
    "    label = row[\"true_label\"]\n",
    "    if dorsal_path and ventral_path:\n",
    "        final_data_list.append({\n",
    "            'image_path': dorsal_path,\n",
    "            'true_label': label,\n",
    "            'sample_id': row['Sample ID'],\n",
    "            'view': 'dorsal',\n",
    "            'sex': row.get('Tick Sex1'),\n",
    "            'life_stage': row.get('Life Stage'),\n",
    "            'attached': row.get('Attached?')\n",
    "        })\n",
    "        final_data_list.append({\n",
    "            'image_path': ventral_path,\n",
    "            'true_label': label,\n",
    "            'sample_id': row['Sample ID'],\n",
    "            'view': 'ventral',\n",
    "            'sex': row.get('Tick Sex1'),\n",
    "            'life_stage': row.get('Life Stage'),\n",
    "            'attached': row.get('Attached?')\n",
    "        })\n",
    "        class_names_set.add(label)\n",
    "    else:\n",
    "        missing_ids.append(row['Sample ID'])\n",
    "\n",
    "print(f\"Created {len(final_data_list)} total entries for the JSON file.\")\n",
    "if missing_ids:\n",
    "    print(f\"Missing images for {len(missing_ids)} specimens: {missing_ids}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee536503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Data manifest saved to: ../data/processed/final_data.json\n",
      "Success! Found 8 unique class names and saved them to: ../data/processed/class_names.json\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Save the JSON File ---\n",
    "output_file = os.path.join(os.path.dirname(FULL_DATA_JSON), \"final_data.json\")\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(final_data_list, f, indent=4)\n",
    "\n",
    "print(f\"Success! Data manifest saved to: {output_file}\")\n",
    "\n",
    "# Convert the class names set to a sorted list for consistent order\n",
    "final_class_names = sorted(list(class_names_set))\n",
    "with open(CLASS_NAMES_JSON, 'w') as f:\n",
    "    json.dump(final_class_names, f, indent=4)\n",
    "\n",
    "print(f\"Success! Found {len(final_class_names)} unique class names and saved them to: {CLASS_NAMES_JSON}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876ffc82",
   "metadata": {},
   "source": [
    "## Getting rid of all the entries with IXODES. Missing species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e9b0a182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Ixodes Filtering ---\n",
      "Found species before filtering: ['Amblyomma americanum', 'Amblyomma maculatum', 'Dermacentor andersoni', 'Dermacentor variabilis', 'Haemaphysalis longicornis', 'Ixodes', 'Ixodes cookei', 'Ixodes scapularis']\n",
      "\n",
      "--- Filtering Results ---\n",
      "Original dataset size: 1324 entries\n",
      "Filtered dataset size: 1160 entries\n",
      "Removed 164 entries\n",
      "\n",
      "Remaining species: ['Amblyomma americanum', 'Amblyomma maculatum', 'Dermacentor andersoni', 'Dermacentor variabilis', 'Haemaphysalis longicornis', 'Ixodes cookei', 'Ixodes scapularis']\n",
      "\n",
      "Saved filtered data to: ../data/processed/final_data_no_ixodes.json\n",
      "Saved filtered class names to: ../data/processed/class_names_no_ixodes.json\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Create filtered JSON without single \"Ixodes\" entries ---\n",
    "EXCLUDE = {\"Ixodes\"}  # Add variations of generic Ixodes\n",
    "\n",
    "print(\"\\n--- Starting Ixodes Filtering ---\")\n",
    "print(f\"Found species before filtering: {sorted(list(class_names_set))}\")\n",
    "\n",
    "# Filter out generic Ixodes entries but keep Ixodes scapularis\n",
    "filtered_data_list = [\n",
    "    entry for entry in final_data_list \n",
    "    if entry['true_label'] not in EXCLUDE\n",
    "]\n",
    "\n",
    "# Save filtered data to new JSON\n",
    "filtered_output_file = FILTERED_DATA_JSON  # Using predefined path\n",
    "with open(filtered_output_file, 'w') as f:\n",
    "    json.dump(filtered_data_list, f, indent=4)\n",
    "\n",
    "# Create new class names list without generic Ixodes\n",
    "filtered_class_names = [name for name in final_class_names if name not in EXCLUDE]\n",
    "with open(FILTERED_CLASS_NAMES_JSON, 'w') as f:  # Using predefined path\n",
    "    json.dump(filtered_class_names, f, indent=4)\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\n--- Filtering Results ---\")\n",
    "print(f\"Original dataset size: {len(final_data_list)} entries\")\n",
    "print(f\"Filtered dataset size: {len(filtered_data_list)} entries\")\n",
    "print(f\"Removed {len(final_data_list) - len(filtered_data_list)} entries\")\n",
    "print(f\"\\nRemaining species: {sorted(filtered_class_names)}\")\n",
    "print(f\"\\nSaved filtered data to: {filtered_output_file}\")\n",
    "print(f\"Saved filtered class names to: {FILTERED_CLASS_NAMES_JSON}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1808ba",
   "metadata": {},
   "source": [
    "## Getting number of Specimens in each resource. Checks for duplicates and missing datapoints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "62b68685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 665 unique Sample IDs in the CSV file.\n",
      "Found 667 unique base IDs from the image filenames.\n",
      "\n",
      "--- DIAGNOSTIC RESULTS ---\n",
      "\n",
      "ðŸš¨ Found 1 IDs in the CSV that have NO matching image base ID:\n",
      "['ZOE-0086-01']\n",
      "------------------------------\n",
      "ðŸš¨ Found 3 image base IDs that are NOT in the CSV:\n",
      "['59-01', 'ZOE-0086-02', 'ZOE-0095-05']\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Get all Sample IDs from the CSV ---\n",
    "# This is our \"source of truth\".\n",
    "df = pd.read_csv(METADATA_PATH)\n",
    "csv_ids = set(df['Sample ID'].astype(str))\n",
    "print(f\"Found {len(csv_ids)} unique Sample IDs in the CSV file.\")\n",
    "\n",
    "# --- Step 2: Get all base IDs from the image filenames ---\n",
    "# This is what's actually in our image folder.\n",
    "image_base_ids = set()\n",
    "for filename in os.listdir(IMAGE_DIR):\n",
    "    # We split \"ZOE-0013-09-01.JPG\" and take the first 3 parts\n",
    "    parts = filename.split('-')\n",
    "    if len(parts) >= 3:\n",
    "        # Rejoin to form the base ID, e.g., \"ZOE-0013-09\"\n",
    "        base_id = '-'.join(parts[:-1]) # Takes all parts except the last one\n",
    "        image_base_ids.add(base_id)\n",
    "\n",
    "print(f\"Found {len(image_base_ids)} unique base IDs from the image filenames.\")\n",
    "\n",
    "# --- Step 3: Find the Mismatches ---\n",
    "# This is the most important part. We find what's in one list but not the other.\n",
    "\n",
    "missing_from_images = csv_ids.difference(image_base_ids)\n",
    "missing_from_csv = image_base_ids.difference(csv_ids)\n",
    "\n",
    "print(\"\\n--- DIAGNOSTIC RESULTS ---\\n\")\n",
    "\n",
    "if not missing_from_images:\n",
    "    print(\"âœ… All Sample IDs from the CSV have a corresponding image base ID.\")\n",
    "else:\n",
    "    print(f\"ðŸš¨ Found {len(missing_from_images)} IDs in the CSV that have NO matching image base ID:\")\n",
    "    # Print the first 10 examples so we can inspect them\n",
    "    print(list(missing_from_images)[:10])\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "if not missing_from_csv:\n",
    "    print(\"âœ… All image base IDs have a corresponding entry in the CSV.\")\n",
    "else:\n",
    "    print(f\"ðŸš¨ Found {len(missing_from_csv)} image base IDs that are NOT in the CSV:\")\n",
    "    # Print the first 10 examples\n",
    "    print(list(missing_from_csv)[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b751f9",
   "metadata": {},
   "source": [
    "## Checking how many Specimens have only 1 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aad18ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- IMAGE COUNT ANALYSIS ---\n",
      "Total unique base IDs found: 667\n",
      "\n",
      " Found 2 specimens with only ONE image:\n",
      "{'246-01': 1, '33-01': 1}\n",
      "\n",
      " Found 1 specimens with MORE than two images:\n",
      "{'435-01': 3}\n"
     ]
    }
   ],
   "source": [
    "# We'll reuse the 'image_base_ids' logic from our diagnostic script,\n",
    "# but this time we'll count the occurrences.\n",
    "from collections import Counter\n",
    "\n",
    "image_id_counts = []\n",
    "for filename in os.listdir(IMAGE_DIR):\n",
    "    parts = filename.split('-')\n",
    "    if len(parts) >= 3:\n",
    "        base_id = '-'.join(parts[:-1])\n",
    "        image_id_counts.append(base_id)\n",
    "\n",
    "# Count how many times each base_id appears\n",
    "id_counts = Counter(image_id_counts)\n",
    "\n",
    "# Now, find the IDs that don't have exactly 2 images\n",
    "specimens_with_one_image = {id: count for id, count in id_counts.items() if count == 1}\n",
    "specimens_with_more_than_two = {id: count for id, count in id_counts.items() if count > 2}\n",
    "\n",
    "\n",
    "print(\"--- IMAGE COUNT ANALYSIS ---\")\n",
    "print(f\"Total unique base IDs found: {len(id_counts)}\")\n",
    "\n",
    "if specimens_with_one_image:\n",
    "    print(f\"\\n Found {len(specimens_with_one_image)} specimens with only ONE image:\")\n",
    "    print(specimens_with_one_image)\n",
    "else:\n",
    "    print(\"\\n All specimens appear to have at least two images.\")\n",
    "\n",
    "if specimens_with_more_than_two:\n",
    "    print(f\"\\n Found {len(specimens_with_more_than_two)} specimens with MORE than two images:\")\n",
    "    print(specimens_with_more_than_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496e9d43",
   "metadata": {},
   "source": [
    "## Matching the images with the available data and building the JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fee10234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_data_list = []\n",
    "# records_with_missing_images = []\n",
    "\n",
    "# # Iterate through each row of the metadata DataFrame.\n",
    "# for index, row in df.iterrows():\n",
    "#     # Get the complete identifier directly from the 'Sample ID' column.\n",
    "#     base_id = row['Sample ID']\n",
    "\n",
    "#     # Construct the expected dorsal and ventral filenames (converted to uppercase for matching).\n",
    "#     dorsal_filename = f\"{base_id}-01.JPG\" or f\"{base_id}-1.JPG\"\n",
    "#     ventral_filename = f\"{base_id}-02.JPG\"\n",
    "\n",
    "#     # Check if both uppercase filenames exist in our set of actual filenames.\n",
    "#     if dorsal_filename in all_image_filenames and ventral_filename in all_image_filenames:\n",
    "        \n",
    "#         # Create the dictionary for the DORSAL image.\n",
    "#         dorsal_entry = {\n",
    "#             'image_path': os.path.join(IMAGE_DIR, dorsal_filename),\n",
    "#             'true_label': row['Species of Tick'],\n",
    "#             'sample_id': row['Sample ID'],\n",
    "#             'view': 'dorsal',\n",
    "#             'sex': row.get('Sex'),\n",
    "#             'life_stage': row.get('Life Stage'),\n",
    "#             'attached': row.get('Attached?')\n",
    "#         }\n",
    "#         final_data_list.append(dorsal_entry)\n",
    "\n",
    "#         # Create the dictionary for the VENTRAL image.\n",
    "#         ventral_entry = {\n",
    "#             'image_path': os.path.join(IMAGE_DIR, ventral_filename),\n",
    "#             'true_label': row['Species of Tick'],\n",
    "#             'sample_id': row['Sample ID'],\n",
    "#             'view': 'ventral',\n",
    "#             'sex': row.get('Sex'),\n",
    "#             'life_stage': row.get('Life Stage'),\n",
    "#             'attached': row.get('Attached?')\n",
    "#         }\n",
    "#         final_data_list.append(ventral_entry)\n",
    "#     else:\n",
    "#         # If one or both images are missing, log the record for review.\n",
    "#         records_with_missing_images.append(base_id)\n",
    "\n",
    "# print(f\"Successfully created {len(final_data_list)} data entries for the JSON file.\")\n",
    "# if records_with_missing_images:\n",
    "#     print(f\"\\nWarning: Could not find image pairs for the following {len(records_with_missing_images)} records:\")\n",
    "#     print(records_with_missing_images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
